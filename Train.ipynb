{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e6ebfe",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# SPDX-FileCopyrightText: Copyright (c) 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n",
    "# SPDX-License-Identifier: LicenseRef-NvidiaProprietary\n",
    "#\n",
    "# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual\n",
    "# property and proprietary rights in and to this material, related\n",
    "# documentation and any modifications thereto. Any use, reproduction,\n",
    "# disclosure or distribution of this material and related documentation\n",
    "# without an express license agreement from NVIDIA CORPORATION or\n",
    "# its affiliates is strictly prohibited."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae097b7",
   "metadata": {},
   "source": [
    "This notebook trains the deep learning-based NPRACH synchronization algorithm from [AIT] considering a 3GPP UMi channel and using the [Sionna link-level simulater](https://nvlabs.github.io/sionna/).\n",
    "\n",
    "[AIT] https://arxiv.org/abs/2205.10805"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9584cf33-7752-48d3-9452-310042cae30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print('Number of GPUs available :', len(gpus))\n",
    "if gpus:\n",
    "    gpu_num = 0 # Index of the GPU to use\n",
    "    try:\n",
    "        tf.config.set_visible_devices(gpus[gpu_num], 'GPU')\n",
    "        print('Only GPU number', gpu_num, 'used.')\n",
    "        tf.config.experimental.set_memory_growth(gpus[gpu_num], True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b0f7a3-5c74-4a1b-a60a-77ff0a3bcd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sionna as sn\n",
    "sn.config.xla_compat = True\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "import datetime\n",
    "\n",
    "from parameters import *\n",
    "from e2e import E2E"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b55fc04-6be4-4da2-b3ec-96bb31979122",
   "metadata": {},
   "source": [
    "## Utility function for saving the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13e934d-3e2b-4d95-a301-2a96f7f866e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_weights(sys):\n",
    "    with open(DEEPNSYNCH_WEIGHTS, \"wb\") as f:\n",
    "        pickle.dump(sys.get_weights(), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f324cf-fbbc-4a9a-8066-a4d6fd4cb191",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558060f2-5a53-48a6-86e3-eb19c32cfb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(sys):\n",
    "    optimizer = tf.optimizers.Adam()\n",
    "    \n",
    "    @tf.function(jit_compile=True)\n",
    "    def training_step():\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Forward pass\n",
    "            loss_tx_ue, loss_toa, loss_cfo = sys(BATCH_SIZE_TRAIN)\n",
    "            # Loss aggregation\n",
    "            loss = loss_tx_ue + loss_toa + loss_cfo\n",
    "        # Compute and apply gradients\n",
    "        grads = tape.gradient(loss, tape.watched_variables())\n",
    "        optimizer.apply_gradients(zip(grads, tape.watched_variables()))\n",
    "        #\n",
    "        return loss_tx_ue, loss_toa, loss_cfo\n",
    "    \n",
    "    current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    test_summary_writer = tf.summary.create_file_writer(f'logs/{current_time}')\n",
    "    with test_summary_writer.as_default():\n",
    "        for i in range(NUM_IT_TRAIN):\n",
    "            loss_tx_ue, loss_toa, loss_cfo = training_step()\n",
    "            # Periodically print update\n",
    "            if (i%128) == 0:\n",
    "                tf.summary.scalar('Det', loss_tx_ue.numpy(), step=i)\n",
    "                tf.summary.scalar('WMSE ToA', loss_toa.numpy(), step=i)\n",
    "                tf.summary.scalar('WMSE CFO', loss_cfo.numpy(), step=i)\n",
    "            # Periodically save the weights\n",
    "            if (i%1024) == 0:\n",
    "                save_weights(sys)\n",
    "\n",
    "    save_weights(sys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f634d7c-349a-40f7-b13c-1049cfc961c5",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8978d0a-e729-40fc-9115-7a69ca0e621d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "sys = E2E('dl', True, nprach_num_rep=NPRACH_NUM_REP, nprach_num_sc=NPRACH_NUM_SC)\n",
    "training_loop(sys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8280baa6-735d-4e9d-8b61-6f54a1613cd7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
