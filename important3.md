با بررسی دقیق مقاله، جزئیات فنی درخواست‌شده به شرح زیر است. برای مواردی که در متن مقاله به صراحت ذکر نشده‌اند، عبارت **«یافت نشد»** قید شده است.

---

### **پارامترهای موج/نمونه‌برداری**

* **N برای ورودی شبکه (اندازه DFT در مسیر شبکه)، و پهنای‌باند مؤثر W=N·Δf در مسیر شبکه:** یافت نشد. مقاله ابعاد تنسور ورودی به شبکه را Y ∈ C^(N×5S) ذکر می‌کند اما مقدار عددی N (تعداد کل زیرحامل‌های پردازش‌شده توسط ایستگاه پایه) را مشخص نکرده است. فقط اشاره شده که برای الگوریتم baseline، اندازه FFT برابر 256 است1.

* **زنجیره دسیماسیون/فیلتر ضد‌روبش از 50 MHz به W:** یافت نشد. مقاله به هیچ‌یک از مراحل فیلترینگ یا کاهش نرخ نمونه‌برداری اشاره‌ای نکرده است.  
* **نگاشت عددی 48 زیرحامل NPRACH به ایندکس‌های DFT:** یافت نشد. مقاله بیان می‌کند که 48 زیرحامل به NPRACH اختصاص داده شده 2 اما ایندکس دقیق آن‌ها در شبکه DFT را مشخص نکرده است.

* **طول‌های نمونه‌ای (N ،N\_SG، نسبت N\_CP/N):** یافت نشد. مقاله طول CP را 66.7 میکروثانیه ذکر می‌کند 3، اما تعداد نمونه‌های متناظر با آن یا سایر پارامترها را ارائه نکرده است.

---

### **الگوی جهش فرکانسی**

* **قاعده صریح تولید φ\_k\[m\] مطابق 3GPP برای فرمت 0:** یافت نشد. مقاله تنها به این نکته بسنده می‌کند که الگوهای جهش طبق مشخصات 3GPP \[2\] تولید می‌شوند 4و از توصیف کیفی «شبه‌تصادفی» فراتر نمی‌رود5. فرمول یا جدول مشخصی ارائه نشده است.

---

### **سناریوی چندکاربری**

* **تعداد کاربران فعال هم‌زمان در هر نمونه (در آموزش و ارزیابی)، و سیاست برخورد/عدم‌برخورد:**  
  * **سیاست برخورد:** سناریو به صورت **بدون برخورد (collisionless)** فرض شده است، به این معنی که کاربران هم‌زمان از الگوهای جهش متفاوتی استفاده می‌کنند6.

  * **تعداد کاربران:** در **آموزش**، احتمال فعال بودن هر کاربر (P(A\_k)) به صورت مستقل و یکنواخت از بازه (0,1) نمونه‌گیری می‌شود7. در

    **ارزیابی** شکل‌های 3a, 3b, 3c، این احتمال ثابت و برابر 0.5 در نظر گرفته شده است8.

* **مقدار دقیق p=P(A\_k) در ارزیابی شکل 3e:** یافت نشد. مقاله مقدار این پارامتر را برای شکل 3e مشخص نکرده است.  
* **نحوه نمونه‌گیری large-scale در آموزش:** یافت نشد. مقاله ذکر می‌کند که پارامترهای large-scale به صورت تصادفی انتخاب می‌شوند تا از بیش‌برازش جلوگیری شود 9 اما توزیع یا دامنه دقیق pathloss و shadowing را مشخص نکرده است.

---

### **مدل کانال UMi و نویز**

* **PDP دقیق (تعداد مسیرها، تاخیر/توان)، احتمال LOS، K-factor:** یافت نشد. مقاله به استفاده از مدل کانال 3GPP UMi مطابق با سند \[8\] اشاره می‌کند 10 اما پارامترهای جزئی آن را در متن بازگو نکرده است.

* **نحوه تنظیم σ² و تعریف عملی SNR\_k:** فرمول دقیق برای SNR\_k در معادله (12) ارائه شده است11:

  SNRk​:=σ2βk​​S1​m=0∑S−1​∣Hk​(Nϕk​\[m\]​)∣2

  این تعریف بر اساس توان ارسالی، واریانس نویز و میانگین بهره کانال است. نحوه تنظیم مستقیم واریانس نویز (σ²) توضیح داده نشده است.  
* **تایید عدم دوبلر (زمان‌نامتغیر) و محل اعمال CFO:** مقاله به صراحت بیان می‌کند که کانال **زمان-نامتغیر (time-invariant)** فرض شده است12. همچنین در معادله (4)، ترم CFO به سیگنال حوزه زمان قبل از اعمال DFT اعمال می‌شود که مدل فیزیکی صحیح را نشان می‌دهد13.

---

### **پیش‌پردازش/نرمال‌سازی ویژگی**

* **فرمول نرمال‌سازی توالی‌ها:** یافت نشد. در فلوچارت عبارت "Layer norm" ذکر شده 1414 اما فرمول دقیق آن (مثلاً تقسیم بر انحراف معیار یا انرژی) ارائه نشده است.

* **فرمول دقیق کانال سوم (log-power):** این کانال از طریق محاسبه میانگین توان هر توالی ē و سپس اعمال log10 بر روی آن به دست می‌آید15151515. جزئیاتی مانند افزودن اپسیلون یا کلیپ کردن مقادیر ذکر نشده است.

* **جزئیات LayerNorm:** یافت نشد. مقاله به وجود لایه‌های LayerNorm در معماری اشاره می‌کند 16161616 اما جزئیات محور اعمال، اپسیلون و ترتیب دقیق آن در بلوک‌ها را مشخص نکرده است.

---

### **اهداف رگرسیونی و معیارها**

* **واحد هدف CFO در آموزش:** هدف رگرسیون (f\_off,k)، **CFO نرمال‌شده بر فرکانس نمونه‌برداری** است17.

* **واحد هدف ToA:** هدف رگرسیون (D\_k)، **زمان رسیدن (ToA) بر حسب ثانیه (یا میکروثانیه)** است. این مقدار از بازه (0, 66.7 µs) نمونه‌برداری می‌شود18.

* **تعریف دقیق "Normalized RMSE":** یافت نشد. مقاله مخرج نرمال‌سازی را برای ToA RMSE و CFO RMSE در شکل‌ها تعریف نکرده است.

---

### **جزئیات معماری**

* **تعداد بلوک‌های ResNet، ابعاد کانال، kernel/stride/padding، نوع Activation:**  
  * **تعداد بلوک‌ها:** فلوچارت معماری، **چهار بلوک ResNet** را در یک مسیر و **دو بلوک** را در مسیر دیگر نشان می‌دهد 19.

  * **جزئیات:** مقاله جزئیات داخلی بلوک‌های ResNet (تعداد لایه‌ها، ابعاد و...) را مشخص نکرده است. برای لایه‌های کانولوشن اولیه، از 128 کرنل با اندازه 3، به همراه zero-padding استفاده شده است20202020. تابع فعال‌سازی در لایه‌های متراکم (Dense) از نوع

    **ReLU** است 21.

* **منظم‌سازها (dropout, weight decay, etc.):** یافت نشد. مقاله به استفاده از هیچ‌یک از این تکنیک‌ها اشاره نکرده است.  
* **محل دقیق اشتراک وزن بین K الگو و نحوه تجمیع روی بُعد S:** مقاله به صراحت بیان می‌کند که **وزن‌ها در بین K الگوی جهش برای شبکه‌های MLP به اشتراک گذاشته می‌شوند**22. تجمیع روی بعد S (زمان) از طریق عملیات

  **Flatten** انجام می‌شود که خروجی کانولوشن‌ها را برای هر الگو به یک بردار تبدیل کرده و به MLP می‌دهد23232323.

---

### **تنظیمات آموزش/ارزیابی**

* **تعداد epoch، طرح LR schedule، معیار early stopping:** یافت نشد. مقاله تنها به نرخ یادگیری ثابت  
  10⁻³ با بهینه‌ساز Adam اشاره کرده است24.

* **اندازه و ترکیب دیتاست‌های train/val/test:** یافت نشد.  
* **seedهای تصادفی برای بازتولید و تعداد نمونه ارزیابی:** یافت نشد.  
* **Grid دقیق نقاط SNR/CFO:** یافت نشد. محدوده متغیرها در نمودارها قابل مشاهده است (مثلاً SNR از 20- تا 20 دسی‌بل 25) اما گام‌های دقیق بین نقاط ذکر نشده است.

---

### **Baseline مرجع و آستانه**

* **روش کالیبراسیون آستانه γ:** یافت نشد. مقاله بیان می‌کند که آستانه برای احتمال‌های آلارم کاذب 99% و 99.9% تنظیم شده 26، اما نحوه استخراج مقدار عددی آستانه از روی این احتمال‌ها را توضیح نداده است.

* **رفع ابهام درباره P\_FA و تعریف دقیق FPR/FNR:** یافت نشد. مقاله از عبارت نامتعارف "false alarm probabilities of 99.9%" استفاده می‌کند که احتمالاً منظور P\_FA برابر 0.1% است، اما این موضوع را شفاف‌سازی نمی‌کند. تعاریف ریاضی FNR و FPR نیز ارائه نشده‌اند.

---

### **برچسب‌گذاری در حضور چندکاربر**

* **مقدار اهداف رگرسیون برای kهای غیرفعال (Ak=0):** یافت نشد. اما این موضوع برای آموزش بی‌اهمیت است، زیرا تابع زیان رگرسیون  
  (L\_2) به وسیله A\_k وزن‌دهی شده و فقط کاربران فعال در محاسبه آن لحاظ می‌شوند27.

* **شیوه دقیق جمع زیان‌ها:** تابع زیان کل به صورت جمع ساده زیان تشخیص (L\_1) و زیان رگرسیون (L\_2) است: L \= L\_1 \+ L\_228. زیان رگرسیون نیز به صورت یک جمع وزن‌دار با

  A\_k و SNR\_k روی تمام K کاربر محاسبه می‌شود29.  
